{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1232,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008116883116883116,
      "grad_norm": 6.651984214782715,
      "learning_rate": 4.971590909090909e-05,
      "loss": 2.0119,
      "step": 10
    },
    {
      "epoch": 0.016233766233766232,
      "grad_norm": 4.509149551391602,
      "learning_rate": 4.9310064935064935e-05,
      "loss": 1.9587,
      "step": 20
    },
    {
      "epoch": 0.024350649350649352,
      "grad_norm": 7.53052282333374,
      "learning_rate": 4.8904220779220784e-05,
      "loss": 1.8744,
      "step": 30
    },
    {
      "epoch": 0.032467532467532464,
      "grad_norm": 4.419536590576172,
      "learning_rate": 4.8498376623376626e-05,
      "loss": 1.8684,
      "step": 40
    },
    {
      "epoch": 0.040584415584415584,
      "grad_norm": 3.2535743713378906,
      "learning_rate": 4.809253246753247e-05,
      "loss": 1.7823,
      "step": 50
    },
    {
      "epoch": 0.048701298701298704,
      "grad_norm": 3.7963173389434814,
      "learning_rate": 4.768668831168832e-05,
      "loss": 1.7765,
      "step": 60
    },
    {
      "epoch": 0.056818181818181816,
      "grad_norm": 3.387179136276245,
      "learning_rate": 4.728084415584416e-05,
      "loss": 1.7785,
      "step": 70
    },
    {
      "epoch": 0.06493506493506493,
      "grad_norm": 3.3197073936462402,
      "learning_rate": 4.6875e-05,
      "loss": 1.7832,
      "step": 80
    },
    {
      "epoch": 0.07305194805194805,
      "grad_norm": 3.7655885219573975,
      "learning_rate": 4.646915584415585e-05,
      "loss": 1.7627,
      "step": 90
    },
    {
      "epoch": 0.08116883116883117,
      "grad_norm": 3.0075507164001465,
      "learning_rate": 4.606331168831169e-05,
      "loss": 1.7239,
      "step": 100
    },
    {
      "epoch": 0.08928571428571429,
      "grad_norm": 3.1717872619628906,
      "learning_rate": 4.5657467532467534e-05,
      "loss": 1.6953,
      "step": 110
    },
    {
      "epoch": 0.09740259740259741,
      "grad_norm": 4.245690822601318,
      "learning_rate": 4.525162337662338e-05,
      "loss": 1.7206,
      "step": 120
    },
    {
      "epoch": 0.10551948051948051,
      "grad_norm": 4.526891708374023,
      "learning_rate": 4.484577922077922e-05,
      "loss": 1.6868,
      "step": 130
    },
    {
      "epoch": 0.11363636363636363,
      "grad_norm": 3.124504327774048,
      "learning_rate": 4.443993506493507e-05,
      "loss": 1.6641,
      "step": 140
    },
    {
      "epoch": 0.12175324675324675,
      "grad_norm": 3.855248212814331,
      "learning_rate": 4.4034090909090916e-05,
      "loss": 1.6702,
      "step": 150
    },
    {
      "epoch": 0.12987012987012986,
      "grad_norm": 3.18314790725708,
      "learning_rate": 4.362824675324675e-05,
      "loss": 1.643,
      "step": 160
    },
    {
      "epoch": 0.137987012987013,
      "grad_norm": 2.91408109664917,
      "learning_rate": 4.32224025974026e-05,
      "loss": 1.6638,
      "step": 170
    },
    {
      "epoch": 0.1461038961038961,
      "grad_norm": 3.3080997467041016,
      "learning_rate": 4.281655844155844e-05,
      "loss": 1.6398,
      "step": 180
    },
    {
      "epoch": 0.15422077922077923,
      "grad_norm": 3.744727373123169,
      "learning_rate": 4.2410714285714285e-05,
      "loss": 1.6309,
      "step": 190
    },
    {
      "epoch": 0.16233766233766234,
      "grad_norm": 3.468404531478882,
      "learning_rate": 4.2004870129870134e-05,
      "loss": 1.6289,
      "step": 200
    },
    {
      "epoch": 0.17045454545454544,
      "grad_norm": 3.3073291778564453,
      "learning_rate": 4.1599025974025976e-05,
      "loss": 1.6109,
      "step": 210
    },
    {
      "epoch": 0.17857142857142858,
      "grad_norm": 3.855922222137451,
      "learning_rate": 4.119318181818182e-05,
      "loss": 1.5981,
      "step": 220
    },
    {
      "epoch": 0.18668831168831168,
      "grad_norm": 4.316303730010986,
      "learning_rate": 4.078733766233767e-05,
      "loss": 1.6276,
      "step": 230
    },
    {
      "epoch": 0.19480519480519481,
      "grad_norm": 3.7116003036499023,
      "learning_rate": 4.038149350649351e-05,
      "loss": 1.6129,
      "step": 240
    },
    {
      "epoch": 0.20292207792207792,
      "grad_norm": 2.752058506011963,
      "learning_rate": 3.997564935064935e-05,
      "loss": 1.6241,
      "step": 250
    },
    {
      "epoch": 0.21103896103896103,
      "grad_norm": 2.722223997116089,
      "learning_rate": 3.95698051948052e-05,
      "loss": 1.6044,
      "step": 260
    },
    {
      "epoch": 0.21915584415584416,
      "grad_norm": 3.4851043224334717,
      "learning_rate": 3.916396103896104e-05,
      "loss": 1.5869,
      "step": 270
    },
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 3.2882981300354004,
      "learning_rate": 3.8758116883116884e-05,
      "loss": 1.5974,
      "step": 280
    },
    {
      "epoch": 0.2353896103896104,
      "grad_norm": 2.7078044414520264,
      "learning_rate": 3.835227272727273e-05,
      "loss": 1.6035,
      "step": 290
    },
    {
      "epoch": 0.2435064935064935,
      "grad_norm": 3.008289098739624,
      "learning_rate": 3.794642857142857e-05,
      "loss": 1.587,
      "step": 300
    },
    {
      "epoch": 0.25162337662337664,
      "grad_norm": 3.03312349319458,
      "learning_rate": 3.754058441558442e-05,
      "loss": 1.6013,
      "step": 310
    },
    {
      "epoch": 0.2597402597402597,
      "grad_norm": 3.8026368618011475,
      "learning_rate": 3.7134740259740266e-05,
      "loss": 1.5699,
      "step": 320
    },
    {
      "epoch": 0.26785714285714285,
      "grad_norm": 2.707216739654541,
      "learning_rate": 3.67288961038961e-05,
      "loss": 1.5484,
      "step": 330
    },
    {
      "epoch": 0.275974025974026,
      "grad_norm": 2.9751663208007812,
      "learning_rate": 3.632305194805195e-05,
      "loss": 1.5543,
      "step": 340
    },
    {
      "epoch": 0.2840909090909091,
      "grad_norm": 3.235258102416992,
      "learning_rate": 3.59172077922078e-05,
      "loss": 1.5316,
      "step": 350
    },
    {
      "epoch": 0.2922077922077922,
      "grad_norm": 2.624343156814575,
      "learning_rate": 3.5511363636363635e-05,
      "loss": 1.5273,
      "step": 360
    },
    {
      "epoch": 0.3003246753246753,
      "grad_norm": 2.271127700805664,
      "learning_rate": 3.5105519480519484e-05,
      "loss": 1.5586,
      "step": 370
    },
    {
      "epoch": 0.30844155844155846,
      "grad_norm": 2.447862386703491,
      "learning_rate": 3.4699675324675326e-05,
      "loss": 1.5513,
      "step": 380
    },
    {
      "epoch": 0.31655844155844154,
      "grad_norm": 2.861872911453247,
      "learning_rate": 3.429383116883117e-05,
      "loss": 1.5301,
      "step": 390
    },
    {
      "epoch": 0.3246753246753247,
      "grad_norm": 2.856135129928589,
      "learning_rate": 3.388798701298702e-05,
      "loss": 1.5403,
      "step": 400
    },
    {
      "epoch": 0.3327922077922078,
      "grad_norm": 2.51564359664917,
      "learning_rate": 3.348214285714286e-05,
      "loss": 1.5324,
      "step": 410
    },
    {
      "epoch": 0.3409090909090909,
      "grad_norm": 2.653033494949341,
      "learning_rate": 3.30762987012987e-05,
      "loss": 1.496,
      "step": 420
    },
    {
      "epoch": 0.349025974025974,
      "grad_norm": 2.7640817165374756,
      "learning_rate": 3.267045454545455e-05,
      "loss": 1.5518,
      "step": 430
    },
    {
      "epoch": 0.35714285714285715,
      "grad_norm": 2.727980613708496,
      "learning_rate": 3.226461038961039e-05,
      "loss": 1.5173,
      "step": 440
    },
    {
      "epoch": 0.3652597402597403,
      "grad_norm": 2.9322803020477295,
      "learning_rate": 3.1858766233766234e-05,
      "loss": 1.53,
      "step": 450
    },
    {
      "epoch": 0.37337662337662336,
      "grad_norm": 2.357823133468628,
      "learning_rate": 3.145292207792208e-05,
      "loss": 1.4892,
      "step": 460
    },
    {
      "epoch": 0.3814935064935065,
      "grad_norm": 2.6914451122283936,
      "learning_rate": 3.1047077922077925e-05,
      "loss": 1.5259,
      "step": 470
    },
    {
      "epoch": 0.38961038961038963,
      "grad_norm": 2.9344894886016846,
      "learning_rate": 3.064123376623377e-05,
      "loss": 1.5293,
      "step": 480
    },
    {
      "epoch": 0.3977272727272727,
      "grad_norm": 2.7849011421203613,
      "learning_rate": 3.0235389610389613e-05,
      "loss": 1.4916,
      "step": 490
    },
    {
      "epoch": 0.40584415584415584,
      "grad_norm": 2.7229673862457275,
      "learning_rate": 2.9829545454545455e-05,
      "loss": 1.4543,
      "step": 500
    },
    {
      "epoch": 0.413961038961039,
      "grad_norm": 2.7330939769744873,
      "learning_rate": 2.94237012987013e-05,
      "loss": 1.4794,
      "step": 510
    },
    {
      "epoch": 0.42207792207792205,
      "grad_norm": 2.4671168327331543,
      "learning_rate": 2.9017857142857146e-05,
      "loss": 1.519,
      "step": 520
    },
    {
      "epoch": 0.4301948051948052,
      "grad_norm": 2.5803308486938477,
      "learning_rate": 2.8612012987012988e-05,
      "loss": 1.4842,
      "step": 530
    },
    {
      "epoch": 0.4383116883116883,
      "grad_norm": 2.7917890548706055,
      "learning_rate": 2.8206168831168834e-05,
      "loss": 1.5116,
      "step": 540
    },
    {
      "epoch": 0.44642857142857145,
      "grad_norm": 2.205610990524292,
      "learning_rate": 2.780032467532468e-05,
      "loss": 1.5103,
      "step": 550
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 2.8357198238372803,
      "learning_rate": 2.739448051948052e-05,
      "loss": 1.4874,
      "step": 560
    },
    {
      "epoch": 0.46266233766233766,
      "grad_norm": 3.1329617500305176,
      "learning_rate": 2.6988636363636367e-05,
      "loss": 1.4684,
      "step": 570
    },
    {
      "epoch": 0.4707792207792208,
      "grad_norm": 2.5753726959228516,
      "learning_rate": 2.6582792207792205e-05,
      "loss": 1.4782,
      "step": 580
    },
    {
      "epoch": 0.4788961038961039,
      "grad_norm": 2.741726875305176,
      "learning_rate": 2.617694805194805e-05,
      "loss": 1.4359,
      "step": 590
    },
    {
      "epoch": 0.487012987012987,
      "grad_norm": 2.8295862674713135,
      "learning_rate": 2.57711038961039e-05,
      "loss": 1.4614,
      "step": 600
    },
    {
      "epoch": 0.49512987012987014,
      "grad_norm": 2.418368339538574,
      "learning_rate": 2.536525974025974e-05,
      "loss": 1.4842,
      "step": 610
    },
    {
      "epoch": 0.5032467532467533,
      "grad_norm": 2.519157886505127,
      "learning_rate": 2.4959415584415584e-05,
      "loss": 1.4684,
      "step": 620
    },
    {
      "epoch": 0.5113636363636364,
      "grad_norm": 2.4328606128692627,
      "learning_rate": 2.455357142857143e-05,
      "loss": 1.4798,
      "step": 630
    },
    {
      "epoch": 0.5194805194805194,
      "grad_norm": 2.523374319076538,
      "learning_rate": 2.4147727272727275e-05,
      "loss": 1.4707,
      "step": 640
    },
    {
      "epoch": 0.5275974025974026,
      "grad_norm": 2.5030412673950195,
      "learning_rate": 2.3741883116883117e-05,
      "loss": 1.4821,
      "step": 650
    },
    {
      "epoch": 0.5357142857142857,
      "grad_norm": 2.27457332611084,
      "learning_rate": 2.3336038961038963e-05,
      "loss": 1.4682,
      "step": 660
    },
    {
      "epoch": 0.5438311688311688,
      "grad_norm": 2.3099403381347656,
      "learning_rate": 2.2930194805194808e-05,
      "loss": 1.474,
      "step": 670
    },
    {
      "epoch": 0.551948051948052,
      "grad_norm": 2.658140182495117,
      "learning_rate": 2.252435064935065e-05,
      "loss": 1.4954,
      "step": 680
    },
    {
      "epoch": 0.560064935064935,
      "grad_norm": 2.6155166625976562,
      "learning_rate": 2.2118506493506492e-05,
      "loss": 1.5013,
      "step": 690
    },
    {
      "epoch": 0.5681818181818182,
      "grad_norm": 2.3928587436676025,
      "learning_rate": 2.171266233766234e-05,
      "loss": 1.4954,
      "step": 700
    },
    {
      "epoch": 0.5762987012987013,
      "grad_norm": 2.0990490913391113,
      "learning_rate": 2.1306818181818183e-05,
      "loss": 1.4623,
      "step": 710
    },
    {
      "epoch": 0.5844155844155844,
      "grad_norm": 2.665944814682007,
      "learning_rate": 2.0900974025974026e-05,
      "loss": 1.4762,
      "step": 720
    },
    {
      "epoch": 0.5925324675324676,
      "grad_norm": 2.3317201137542725,
      "learning_rate": 2.049512987012987e-05,
      "loss": 1.485,
      "step": 730
    },
    {
      "epoch": 0.6006493506493507,
      "grad_norm": 2.476588010787964,
      "learning_rate": 2.0089285714285717e-05,
      "loss": 1.4941,
      "step": 740
    },
    {
      "epoch": 0.6087662337662337,
      "grad_norm": 2.2554728984832764,
      "learning_rate": 1.968344155844156e-05,
      "loss": 1.4521,
      "step": 750
    },
    {
      "epoch": 0.6168831168831169,
      "grad_norm": 2.408867120742798,
      "learning_rate": 1.9277597402597404e-05,
      "loss": 1.4653,
      "step": 760
    },
    {
      "epoch": 0.625,
      "grad_norm": 2.552657127380371,
      "learning_rate": 1.887175324675325e-05,
      "loss": 1.4356,
      "step": 770
    },
    {
      "epoch": 0.6331168831168831,
      "grad_norm": 2.975214958190918,
      "learning_rate": 1.8465909090909092e-05,
      "loss": 1.4352,
      "step": 780
    },
    {
      "epoch": 0.6412337662337663,
      "grad_norm": 2.3744208812713623,
      "learning_rate": 1.8060064935064934e-05,
      "loss": 1.4555,
      "step": 790
    },
    {
      "epoch": 0.6493506493506493,
      "grad_norm": 2.4324772357940674,
      "learning_rate": 1.7654220779220783e-05,
      "loss": 1.4644,
      "step": 800
    },
    {
      "epoch": 0.6574675324675324,
      "grad_norm": 3.1685097217559814,
      "learning_rate": 1.7248376623376625e-05,
      "loss": 1.4477,
      "step": 810
    },
    {
      "epoch": 0.6655844155844156,
      "grad_norm": 2.502650499343872,
      "learning_rate": 1.6842532467532467e-05,
      "loss": 1.4368,
      "step": 820
    },
    {
      "epoch": 0.6737012987012987,
      "grad_norm": 2.724336624145508,
      "learning_rate": 1.6436688311688313e-05,
      "loss": 1.4124,
      "step": 830
    },
    {
      "epoch": 0.6818181818181818,
      "grad_norm": 2.3135194778442383,
      "learning_rate": 1.6030844155844158e-05,
      "loss": 1.4376,
      "step": 840
    },
    {
      "epoch": 0.689935064935065,
      "grad_norm": 2.7111101150512695,
      "learning_rate": 1.5625e-05,
      "loss": 1.443,
      "step": 850
    },
    {
      "epoch": 0.698051948051948,
      "grad_norm": 2.861832857131958,
      "learning_rate": 1.5219155844155844e-05,
      "loss": 1.4242,
      "step": 860
    },
    {
      "epoch": 0.7061688311688312,
      "grad_norm": 2.5043389797210693,
      "learning_rate": 1.481331168831169e-05,
      "loss": 1.4558,
      "step": 870
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 2.2340426445007324,
      "learning_rate": 1.4407467532467533e-05,
      "loss": 1.4555,
      "step": 880
    },
    {
      "epoch": 0.7224025974025974,
      "grad_norm": 2.069375991821289,
      "learning_rate": 1.4001623376623377e-05,
      "loss": 1.4135,
      "step": 890
    },
    {
      "epoch": 0.7305194805194806,
      "grad_norm": 3.062642812728882,
      "learning_rate": 1.359577922077922e-05,
      "loss": 1.3982,
      "step": 900
    },
    {
      "epoch": 0.7386363636363636,
      "grad_norm": 2.555424928665161,
      "learning_rate": 1.3189935064935067e-05,
      "loss": 1.4312,
      "step": 910
    },
    {
      "epoch": 0.7467532467532467,
      "grad_norm": 2.8495750427246094,
      "learning_rate": 1.2784090909090909e-05,
      "loss": 1.4215,
      "step": 920
    },
    {
      "epoch": 0.7548701298701299,
      "grad_norm": 2.28560209274292,
      "learning_rate": 1.2378246753246754e-05,
      "loss": 1.3982,
      "step": 930
    },
    {
      "epoch": 0.762987012987013,
      "grad_norm": 2.4437122344970703,
      "learning_rate": 1.1972402597402598e-05,
      "loss": 1.4163,
      "step": 940
    },
    {
      "epoch": 0.7711038961038961,
      "grad_norm": 2.198007345199585,
      "learning_rate": 1.1566558441558442e-05,
      "loss": 1.431,
      "step": 950
    },
    {
      "epoch": 0.7792207792207793,
      "grad_norm": 2.2771875858306885,
      "learning_rate": 1.1160714285714287e-05,
      "loss": 1.4517,
      "step": 960
    },
    {
      "epoch": 0.7873376623376623,
      "grad_norm": 2.402827501296997,
      "learning_rate": 1.075487012987013e-05,
      "loss": 1.4116,
      "step": 970
    },
    {
      "epoch": 0.7954545454545454,
      "grad_norm": 2.68489146232605,
      "learning_rate": 1.0349025974025975e-05,
      "loss": 1.4421,
      "step": 980
    },
    {
      "epoch": 0.8035714285714286,
      "grad_norm": 2.359800338745117,
      "learning_rate": 9.943181818181819e-06,
      "loss": 1.4256,
      "step": 990
    },
    {
      "epoch": 0.8116883116883117,
      "grad_norm": 2.570880174636841,
      "learning_rate": 9.537337662337663e-06,
      "loss": 1.4218,
      "step": 1000
    },
    {
      "epoch": 0.8198051948051948,
      "grad_norm": 2.568406820297241,
      "learning_rate": 9.131493506493508e-06,
      "loss": 1.4416,
      "step": 1010
    },
    {
      "epoch": 0.827922077922078,
      "grad_norm": 2.162670612335205,
      "learning_rate": 8.72564935064935e-06,
      "loss": 1.4349,
      "step": 1020
    },
    {
      "epoch": 0.836038961038961,
      "grad_norm": 2.249577045440674,
      "learning_rate": 8.319805194805196e-06,
      "loss": 1.4154,
      "step": 1030
    },
    {
      "epoch": 0.8441558441558441,
      "grad_norm": 2.363443613052368,
      "learning_rate": 7.91396103896104e-06,
      "loss": 1.4387,
      "step": 1040
    },
    {
      "epoch": 0.8522727272727273,
      "grad_norm": 2.403137445449829,
      "learning_rate": 7.5081168831168834e-06,
      "loss": 1.4177,
      "step": 1050
    },
    {
      "epoch": 0.8603896103896104,
      "grad_norm": 2.296259880065918,
      "learning_rate": 7.102272727272728e-06,
      "loss": 1.4209,
      "step": 1060
    },
    {
      "epoch": 0.8685064935064936,
      "grad_norm": 2.66032338142395,
      "learning_rate": 6.696428571428572e-06,
      "loss": 1.4524,
      "step": 1070
    },
    {
      "epoch": 0.8766233766233766,
      "grad_norm": 2.13870906829834,
      "learning_rate": 6.290584415584417e-06,
      "loss": 1.4211,
      "step": 1080
    },
    {
      "epoch": 0.8847402597402597,
      "grad_norm": 2.8848698139190674,
      "learning_rate": 5.88474025974026e-06,
      "loss": 1.3877,
      "step": 1090
    },
    {
      "epoch": 0.8928571428571429,
      "grad_norm": 2.5548806190490723,
      "learning_rate": 5.478896103896104e-06,
      "loss": 1.416,
      "step": 1100
    },
    {
      "epoch": 0.900974025974026,
      "grad_norm": 1.7280257940292358,
      "learning_rate": 5.073051948051948e-06,
      "loss": 1.4362,
      "step": 1110
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 2.4823923110961914,
      "learning_rate": 4.667207792207792e-06,
      "loss": 1.3914,
      "step": 1120
    },
    {
      "epoch": 0.9172077922077922,
      "grad_norm": 2.2957050800323486,
      "learning_rate": 4.2613636363636365e-06,
      "loss": 1.4716,
      "step": 1130
    },
    {
      "epoch": 0.9253246753246753,
      "grad_norm": 2.8413360118865967,
      "learning_rate": 3.855519480519481e-06,
      "loss": 1.4172,
      "step": 1140
    },
    {
      "epoch": 0.9334415584415584,
      "grad_norm": 2.2922539710998535,
      "learning_rate": 3.449675324675325e-06,
      "loss": 1.4094,
      "step": 1150
    },
    {
      "epoch": 0.9415584415584416,
      "grad_norm": 2.0036158561706543,
      "learning_rate": 3.043831168831169e-06,
      "loss": 1.4094,
      "step": 1160
    },
    {
      "epoch": 0.9496753246753247,
      "grad_norm": 2.1090548038482666,
      "learning_rate": 2.637987012987013e-06,
      "loss": 1.4272,
      "step": 1170
    },
    {
      "epoch": 0.9577922077922078,
      "grad_norm": 2.289428234100342,
      "learning_rate": 2.2321428571428573e-06,
      "loss": 1.3938,
      "step": 1180
    },
    {
      "epoch": 0.9659090909090909,
      "grad_norm": 2.250576972961426,
      "learning_rate": 1.8262987012987013e-06,
      "loss": 1.4076,
      "step": 1190
    },
    {
      "epoch": 0.974025974025974,
      "grad_norm": 2.1644656658172607,
      "learning_rate": 1.4204545454545456e-06,
      "loss": 1.3932,
      "step": 1200
    },
    {
      "epoch": 0.9821428571428571,
      "grad_norm": 2.571805715560913,
      "learning_rate": 1.0146103896103896e-06,
      "loss": 1.4293,
      "step": 1210
    },
    {
      "epoch": 0.9902597402597403,
      "grad_norm": 2.3239588737487793,
      "learning_rate": 6.087662337662339e-07,
      "loss": 1.4292,
      "step": 1220
    },
    {
      "epoch": 0.9983766233766234,
      "grad_norm": 2.5294384956359863,
      "learning_rate": 2.029220779220779e-07,
      "loss": 1.4246,
      "step": 1230
    }
  ],
  "logging_steps": 10,
  "max_steps": 1232,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 10000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 321786948354048.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
