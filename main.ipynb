{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of character in dataset:  3934592\n"
     ]
    }
   ],
   "source": [
    "print(\"length of character in dataset: \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets look at first 1000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§¨‡§∞‡•ç‡§¶‡§ø‡§¨‡§æ‡§∏ ‡§®‡§ó‡§∞‡§™‡§æ‡§≤‡§ø‡§ï‡§æ‡§ï‡•ã ‡§§‡•á‡§∏‡•ç‡§∞‡•ã ‡§®‡§ó‡§∞ ‡§™‡§∞‡§ø‡§∑‡§¶‡§¨‡§æ‡§ü ‡§™‡§æ‡§∞‡§ø‡§§ ‡§Ü.‡§µ.‡•®‡•¶‡•≠‡•©‡•§‡•≠‡•™ ‡§ï‡•ã ‡§∏‡§Ç‡§∂‡•ã‡§ß‡§ø‡§§ ‡§∞ ‡•®‡•¶‡•≠‡•™‡•§‡•≠‡•´ ‡§ï‡•ã ‡§™‡•ç‡§∞‡§∏‡•ç‡§§‡§æ‡§µ‡§ø‡§§ ‡§®‡•Ä‡§§‡§ø, ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ ‡§§‡§•‡§æ ‡§¨‡§ú‡•á‡§ü\\n‡§Ö‡§æ‡§∞‡•ç‡§•‡§ø‡§ï ‡§µ‡§∞‡•ç‡§∑ ‡•®‡•¶‡•≠‡•´/‡•≠‡•¨ ‡§ï‡§æ‡•á ‡§®‡§¶‡§ø‡§ú‡§®‡•ç‡§Ø ‡§™‡§¶‡§æ‡§∞‡•ç‡§•‡§ï‡§æ‡•á ‡§â‡§§‡•ç‡§ñ‡§®‡§®‡•ç ‡§ó‡§∞‡•Ä ‡§¨‡§ø‡§ï‡•ç‡§∞‡§ø ‡§µ‡§ø‡§§‡§∞‡§£ ‡§§‡§•‡§æ ‡§Ö‡§æ‡§®‡•ç‡§§‡§∞‡§ø‡§ï ‡§®‡§ø‡§ï‡§æ‡§∏‡•Ä ‡§ó‡§∞‡•ç‡§®‡•á ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡§æ‡•á ‡§¨‡§æ‡•á‡§≤‡§™‡§§‡•ç‡§∞ ‡§∏‡§Æ‡•ç‡§¨‡§®‡•ç‡§ß‡•Ä ‡§∏‡•Å‡§ö‡§®‡§æ\\n‡§∏‡§ï‡•ç‡§∑‡§æ‡§∞ ‡§∏‡§™‡•ç‡§§‡§∞‡•Ä ‡§Ö‡§≠‡§ø‡§Ø‡§æ‡§®‡§Æ‡§æ ‡§∏‡§™‡•ç‡§§‡§∞‡•Ä‡§¨‡§æ‡§∏‡•Ä ‡§∏‡§Æ‡•ç‡§™‡•Ç‡§∞‡•ç‡§£ ‡§∏‡§∞‡•ã‡§ï‡§æ‡§∞‡§µ‡§æ‡§≤‡§æ‡§π‡§∞‡•Å‡§ï‡•ã ‡§∏‡§π‡§Ø‡•ã‡§ó ‡§∞ ‡§∏‡§π‡§≠‡§æ‡§ó‡§ø‡§§‡§æ‡§ï‡§æ‡•ã ‡§≤‡§æ‡§ó‡§ø ‡§Ö‡§®‡•Å‡§∞‡§æ‡•ã‡§ß ‡§õ ‡•§|| ‡§∏‡§æ‡§Æ‡•Å‡§¶‡§æ‡§Ø‡§ø‡§ï ‡§Ö‡§ß‡•ç‡§Ø‡§Ø‡§® ‡§ï‡•á‡§®‡•ç‡§¶‡•ç‡§∞‡§π‡§∞‡•Ç‡§ï‡•ã ‡§®‡§µ‡§ø‡§ï‡§∞‡§£ ‡§∏‡§Æ‡•ç‡§¨‡§®‡•ç‡§ß‡§Æ‡§æ ‡•§||\\n‡§ï‡§æ‡§†‡§Æ‡§æ‡§°‡•å‡§Ç, ‡•ß‡•® ‡§ï‡§æ‡§§‡§ø‡§ï ‡•§ ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§™‡§§‡§ø ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§¶‡•á‡§µ‡•Ä ‡§≠‡§£‡•ç‡§°‡§æ‡§∞‡•Ä ‡§Æ‡§ø‡§§‡•ç‡§∞‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§ï‡§§‡§æ‡§∞‡§ï‡•ã ‡§ö‡§æ‡§∞ ‡§¶‡§ø‡§µ‡§∏‡•Ä‡§Ø ‡§î‡§™‡§ö‡§æ‡§∞‡§ø‡§ï ‡§≠‡•ç‡§∞‡§Æ‡§£‡§Æ‡§æ ‡§Ü‡§ú ‡§§‡•ç‡§Ø‡§∏‡§§‡§∞‡•ç‡§´ ‡§™‡•ç‡§∞‡§∏‡•ç‡§•‡§æ‡§® ‡§ó‡§∞‡•á‡§ï‡•Ä ‡§õ‡§®‡•ç ‡•§ ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§™‡§§‡§ø ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§¶‡•á‡§µ‡•Ä ‡§≠‡§£‡•ç‡§°‡§æ‡§∞‡•Ä ‡§ï‡§§‡§æ‡§∞‡§ï‡§æ ‡§Ö‡§Æ‡§ø‡§∞ ‡§∂‡•á‡§ñ ‡§π‡§Æ‡§æ‡§¶ ‡§¨‡•Ä‡§® ‡§ñ‡§æ‡§≤‡§ø‡§¶‡§æ ‡§Ö‡§≤ ‡§•‡§æ‡§®‡•Ä‡§ï‡•ã ‡§Æ‡•à‡§§‡•ç‡§∞‡•Ä‡§™‡•Ç‡§∞‡•ç‡§£ ‡§®‡§ø‡§Æ‡§®‡•ç‡§§‡•ç‡§∞‡§£‡§æ‡§Æ‡§æ ‡§ö‡§æ‡§∞ ‡§¶‡§ø‡§µ‡§∏‡•Ä‡§Ø ‡§î‡§™‡§ö‡§æ‡§∞‡§ø‡§ï\\n‡§ï‡§æ‡§†‡§Æ‡§æ‡§°‡•å‡§Å, ‡•®‡•¨ ‡§ï‡§æ‡§§‡•ç‡§§‡§ø‡§ï ‡•§ ‡§∏‡§∞‡§ï‡§æ‡§∞‡§≤‡•á ‡§∏‡§ô‡•ç‡§ò, ‡§™‡•ç‡§∞‡§¶‡•á‡§∂ ‡§∞ ‡§∏‡•ç‡§•‡§æ‡§®‡•Ä‡§Ø ‡§§‡§π‡§Æ‡§æ ‡§ï‡§∞‡•ç‡§Æ‡§ö‡§æ‡§∞‡•Ä ‡§∏‡§Æ‡§æ‡§Ø‡•ã‡§ú‡§® ‡§ó‡§∞‡•ç‡§®‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø ‚Äò‡§ï‡§∞‡•ç‡§Æ‡§ö‡§æ‡§∞‡•Ä ‡§∏‡§Æ‡§æ‡§Ø‡•ã‡§ú‡§® ‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§¶‡•á‡§∂‚Äì‡•®‡•¶‡•≠‡•´‚Äô ‡§≤‡•ç‡§Ø‡§æ‡§â‡§®‡•á ‡§§‡§Ø‡§æ‡§∞‡•Ä ‡§ó‡§∞‡•á‡§ï‡•ã ‡§õ ‡•§ ‡§∏‡§∞‡§ï‡§æ‡§∞‡§≤‡•á ‡§Ø‡§∏‡§Ö‡§ò‡§ø ‡§≤‡•ç‡§Ø‡§æ‡§è‡§ï‡•ã\\n‡§ï‡§æ‡§†‡§Æ‡§æ‡§°‡•å‡§Ç, ‡•®‡•¨ ‡§ï‡§æ‡§§‡§ø‡§ï ‡•§ ‡§Æ‡§π‡§æ‡§®‡§æ‡§Ø‡§ï ‡§∞‡§æ‡§ú‡•á‡§∂ ‡§π‡§Æ‡§æ‡§≤ ‡§Ö‡§π‡§ø‡§≤‡•á ‡§ö‡§≤‡§ö‡§ø‡§§‡•ç‡§∞ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡§Æ‡§æ ‡§™‡§æ‡§§‡§≤‡§ø‡§è ‡§™‡§®‡§ø ‡§â‡§®‡§ï‡•ã ‡§∏‡§ø‡§®‡•á ‡§ú‡§ó‡§§‡§Æ‡§æ ‡§®‡§æ‡§Æ‡•à ‡§ï‡§æ‡§´‡•Ä ‡§õ ‡•§ ‡§ï‡•Å‡§®‡•à ‡§∏‡§Æ‡§Ø ‡§¨‡§≤‡§ø‡§â‡§° ‡§∏‡•Å‡§™‡§∞‡§∏‡•ç‡§ü‡§æ‡§∞ ‡§Ö‡§Æ‡§ø‡§§‡§æ‡§≠ ‡§µ‡§ö‡•ç‡§ö‡§®‡§∏‡§Å‡§ó\\n‡§ï‡§æ‡§†‡§Æ‡§æ‡§°‡•å‡§Ç, ‡•®‡•¨ ‡§ï‡§æ‡§§‡§ø‡§ï ‡•§ ‡§Ø‡§Æ‡§®‡§ï‡•ã ‡§™‡•ç‡§∞‡§Æ‡•Å‡§ñ ‡§∂‡§π‡§∞ ‡§π‡•ã‡§°‡•á‡§°‡§æ‡§Æ'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', '\\x17', ' ', '!', '\"', '#', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x96', '¬§', '¬•', '¬©', '¬´', '¬∞', '¬∑', '¬ª', '√ó', '√†', '√°', '√£', '√§', '√•', '√ß', '√©', '√™', '√´', '√¨', '√≠', '√≤', '√≥', '√µ', '√∂', '√∑', 'ƒÅ', 'ƒÖ', 'ƒá', 'ƒô', '≈Ç', '≈Ñ', '≈í', '≈ì', '≈õ', '≈º', '…ô', 'ÀÜ', 'Àà', 'Àê', 'œÄ', '–ê', '–ë', '–í', '–ì', '–î', '–ï', '–ñ', '–ó', '–ò', '–ö', '–õ', '–ú', '–ù', '–û', '–ü', '–†', '–°', '–¢', '–£', '–§', '–•', '–¶', '–®', '–≠', '–Ø', '–∞', '–±', '–≤', '–≥', '–¥', '–µ', '–∂', '–∑', '–∏', '–π', '–∫', '–ª', '–º', '–Ω', '–æ', '–ø', '—Ä', '—Å', '—Ç', '—É', '—Ñ', '—Ö', '—Ü', '—á', '—à', '—â', '—ä', '—ã', '—å', '—ç', '—é', '—è', 'ÿå', 'ÿ¢', 'ÿß', 'ÿ©', 'ÿ±', 'ÿ¥', 'ŸÑ', 'ŸÖ', 'ŸÜ', 'Ÿá', 'Ÿà', 'Ÿä', 'Ÿæ', 'Ÿø', '€æ', '‡§Å', '‡§Ç', '‡§É', '‡§Ö', '‡§Ü', '‡§á', '‡§à', '‡§â', '‡§ä', '‡§ã', '‡§è', '‡§ê', '‡§ë', '‡§ì', '‡§î', '‡§ï', '‡§ñ', '‡§ó', '‡§ò', '‡§ô', '‡§ö', '‡§õ', '‡§ú', '‡§ù', '‡§û', '‡§ü', '‡§†', '‡§°', '‡§¢', '‡§£', '‡§§', '‡§•', '‡§¶', '‡§ß', '‡§®', '‡§™', '‡§´', '‡§¨', '‡§≠', '‡§Æ', '‡§Ø', '‡§∞', '‡§≤', '‡§µ', '‡§∂', '‡§∑', '‡§∏', '‡§π', '‡§º', '‡§Ω', '‡§æ', '‡§ø', '‡•Ä', '‡•Å', '‡•Ç', '‡•É', '‡•Ö', '‡•Ü', '‡•á', '‡•à', '‡•â', '‡•ä', '‡•ã', '‡•å', '‡•ç', '‡•ê', '‡•ú', '‡•ù', '‡•û', '‡•†', '‡•¢', '‡•£', '‡•§', '‡•¶', '‡•ß', '‡•®', '‡•©', '‡•™', '‡•´', '‡•¨', '‡•≠', '‡•Æ', '‡•Ø', '‡•∞', '\\u200a', '\\u200b', '\\u200c', '\\u200d', '‚Äì', '‚Äî', '‚Äò', '‚Äô', '‚Äú', '‚Äù', '‚Ä†', '‚Ä¢', '‚Ä¶', '\\u202f', '‚Ä≤', '‚Ä≥', '‚Üê', '‚Üí', '‚àï', '‚âà', '‚ñ∫', '‚òÖ', '\\u3000', '„ÄÅ', '„ÄÇ', '„Äå', '„Äç', '„ÇÜ', '„Çì', '„Éª', '‰∏Ä', '‰∏≠', '‰∏ª', '‰πã', '‰∫Ü', '‰∫ã', '‰∫í', '‰∫õ', '‰∫§', '‰∫∫', '‰ªã', '‰ªñ', '‰ª•', '‰Ωç', '‰Ωï', '‰æÜ', '‰øÉ', 'ÂÄã', 'ÂÄë', 'ÂÉè', 'ÂÖ•', 'ÂÖ´', 'ÂÖ∂', 'ÂàÜ', 'Âãï', 'Âåñ', 'ÂçÅ', 'Âèä', 'Âèã', 'Âèç', 'ÂèØ', 'Âè≤', 'Âêà', 'Âíå', 'Âõ†', 'Âú®', 'Âú∞', 'Âûã', 'Â£Å', 'Â£´', 'Â§ñ', 'Â§ö', 'Â§ß', 'Â¶Ç', 'Â≠ê', 'ÂÆö', 'ÂÆ∂', 'ÂØ¶', 'Â∞ë', 'Â±Ö', 'Â±ï', 'Â∑Æ', 'Â∑≤', 'Â∏å', 'Âπ¥', 'Â∫≠', 'Âºè', 'ÂΩ¢', 'ÂΩº', 'ÊÄù', 'ÊÉÖ', 'ÊÖã', 'Êáâ', 'Êàê', 'Êàë', 'Êâç', 'Êâì', 'ÊãÜ', 'Êãç', 'Êí≠', 'Êìî', 'Êîù', 'Êîæ', 'ÊïÖ', 'Êï∏', 'Êñá', 'Êóè', 'Êó©', 'ÊòØ', 'ÊúÉ', 'Êúâ', 'Êúã', 'Êúõ', 'Ê†π', 'Ê§ç', 'Ê≠§', 'Ê≠ß', 'Ê≠∑', 'Ê≥Å', 'Ê≥®', 'ÊµÅ', 'Ê∂à', 'Ê∏Ø', 'ÁÇ∫', 'Áà∂', 'ÁâÜ', 'Áâá', 'ÁãÄ', 'Áîü', 'Áï∞', 'Áï∂', 'Áôº', 'ÁöÑ', 'ÁõÆ', 'Áõ¥', 'Áõ∏', 'Áúæ', 'Á†¥', 'Á§æ', 'Á•ñ', 'ÁØÄ', 'ÁØâ', 'Á∞°', 'Á±ç', 'Á¥Ä', 'Á∂ì', 'Á∑ö', 'ËÄå', 'ËÜú', 'Ëàá', 'Ëâ≤', 'ËêΩ', 'Ëôï', 'Ëûç', 'Ë¢´', 'Ë£î', 'Ë¶Å', 'Ë¶ñ', 'ËßÄ', 'Ëßí', 'Ëß£', 'Ë®Ä', 'Ë®ò', 'Ë™û', 'Ë≠∞', 'ËÆì', 'Ëµ∑', 'Ëº©', 'ÈÄô', 'ÈÄ≤', 'ÈÅì', 'Èáç', 'ÈåÑ', 'Èñì', 'Èóú', 'Èô§', 'Èöî', 'ÈõÜ', 'È°å', 'È°û', 'È¶ô', '\\ufeff', 'Ôºå', 'Ôºö', 'ÔΩú', 'ÔøΩ', 'üòâ', 'üòä', 'üòç']\n",
      "479\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(chars)\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71, 72, 72, 3, 83, 71, 68, 81, 68]\n",
      "hii there\n",
      "[236, 241, 256, 212, 3, 221, 252, 270, 236, 268, 3, 253, 259, 240, 259, 253, 259, 240, 270, 227]\n",
      "‡§§‡§™‡§æ‡§à ‡§ï‡§∏‡•ç‡§§‡•ã ‡§π‡•Å‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))\n",
    "\n",
    "print(encode(\"‡§§‡§™‡§æ‡§à ‡§ï‡§∏‡•ç‡§§‡•ã ‡§π‡•Å‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ\"))\n",
    "print(decode(encode(\"‡§§‡§™‡§æ‡§à ‡§ï‡§∏‡•ç‡§§‡•ã ‡§π‡•Å‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3934592]) torch.int64\n",
      "tensor([243, 247, 270, 238, 257, 243, 256, 252,   3, 240, 223, 247, 241, 256,\n",
      "        248, 257, 221, 256, 221, 268,   3, 236, 264, 252, 270, 247, 268,   3,\n",
      "        240, 223, 247,   3, 241, 247, 257, 251, 238, 243, 256, 231,   3, 241,\n",
      "        256, 247, 257, 236,   3, 210,  16, 249,  16, 281, 279, 286, 282, 278,\n",
      "        286, 283,   3, 221, 268,   3, 252, 207, 250, 268, 239, 257, 236,   3,\n",
      "        247,   3, 281, 279, 286, 283, 278, 286, 284,   3, 221, 268,   3, 241,\n",
      "        270, 247, 252, 270, 236, 256, 249, 257, 236,   3, 240, 258, 236, 257,\n",
      "         14,   3, 221, 256, 247, 270, 246, 221, 270, 247, 245,   3, 236, 237,\n",
      "        256,   3, 243, 228, 264, 231,   1, 209, 256, 247, 270, 237, 257, 221,\n",
      "          3, 249, 247, 270, 251,   3, 281, 279, 286, 284,  17, 286, 285,   3,\n",
      "        221, 256, 264,   3, 240, 238, 257, 228, 240, 270, 246,   3, 241, 238,\n",
      "        256, 247, 270, 237, 221, 256, 264,   3, 213, 236, 270, 222, 240, 240,\n",
      "        270,   3, 223, 247, 258,   3, 243, 257, 221, 270, 247, 257,   3, 249,\n",
      "        257, 236, 247, 235,   3, 236, 237, 256,   3, 209, 256, 240, 270, 236,\n",
      "        247, 257, 221,   3, 240, 257, 221, 256, 252, 258,   3, 223, 247, 270,\n",
      "        240, 264,   3, 221, 256, 247, 270, 246, 221, 256, 264,   3, 243, 256,\n",
      "        264, 248, 241, 236, 270, 247,   3, 252, 245, 270, 243, 240, 270, 239,\n",
      "        258,   3, 252, 259, 226, 240, 256,   1, 252, 221, 270, 251, 256, 247,\n",
      "          3, 252, 241, 270, 236, 247, 258,   3, 209, 244, 257, 246, 256, 240,\n",
      "        245, 256,   3, 252, 241, 270, 236, 247, 258, 243, 256, 252, 258,   3,\n",
      "        252, 245, 270, 241, 260, 247, 270, 235,   3, 252, 247, 268, 221, 256,\n",
      "        247, 249, 256, 248, 256, 253, 247, 259, 221, 268,   3, 252, 253, 246,\n",
      "        268, 223,   3, 247,   3, 252, 253, 244, 256, 223, 257, 236, 256, 221,\n",
      "        256, 268,   3, 248, 256, 223, 257,   3, 209, 240, 259, 247, 256, 268,\n",
      "        239,   3, 227,   3, 278,  91,  91,   3, 252, 256, 245, 259, 238, 256,\n",
      "        246, 257, 221,   3, 209, 239, 270, 246, 246, 240,   3, 221, 264, 240,\n",
      "        270, 238, 270, 247, 253, 247, 260, 221, 268,   3, 240, 249, 257, 221,\n",
      "        247, 235,   3, 252, 245, 270, 243, 240, 270, 239, 245, 256,   3, 278,\n",
      "         91,  91,   1, 221, 256, 232, 245, 256, 233, 269, 207,  14,   3, 280,\n",
      "        281,   3, 221, 256, 236, 257, 221,   3, 278,   3, 247, 256, 251, 270,\n",
      "        231, 270, 247, 241, 236, 257,   3, 249, 257, 238, 270, 246, 256, 238,\n",
      "        264, 249, 258,   3, 244, 235, 270, 233, 256, 247, 258,   3, 245, 257,\n",
      "        236, 270, 247, 247, 256, 251, 270, 231, 270, 247,   3, 221, 236, 256,\n",
      "        247, 221, 268,   3, 226, 256, 247,   3, 238, 257, 249, 252, 258, 246,\n",
      "          3, 220, 241, 226, 256, 247, 257, 221,   3, 244, 270, 247, 245, 235,\n",
      "        245, 256,   3, 210, 228,   3, 236, 270, 246, 252, 236, 247, 270, 242,\n",
      "          3, 241, 270, 247, 252, 270, 237, 256, 240,   3, 223, 247, 264, 221,\n",
      "        258,   3, 227, 240, 270,   3, 278,   3, 247, 256, 251, 270, 231, 270,\n",
      "        247, 241, 236, 257,   3, 249, 257, 238, 270, 246, 256, 238, 264, 249,\n",
      "        258,   3, 244, 235, 270, 233, 256, 247, 258,   3, 221, 236, 256, 247,\n",
      "        221, 256,   3, 209, 245, 257, 247,   3, 250, 264, 222,   3, 253, 245,\n",
      "        256, 238,   3, 243, 258, 240,   3, 222, 256, 248, 257, 238, 256,   3,\n",
      "        209, 248,   3, 237, 256, 240, 258, 221, 268,   3, 245, 265, 236, 270,\n",
      "        247, 258, 241, 260, 247, 270, 235,   3, 240, 257, 245, 240, 270, 236,\n",
      "        270, 247, 235, 256, 245, 256,   3, 226, 256, 247,   3, 238, 257, 249,\n",
      "        252, 258, 246,   3, 220, 241, 226, 256, 247, 257, 221,   1, 221, 256,\n",
      "        232, 245, 256, 233, 269, 206,  14,   3, 281, 285,   3, 221, 256, 236,\n",
      "        270, 236, 257, 221,   3, 278,   3, 252, 247, 221, 256, 247, 248, 264,\n",
      "          3, 252, 225, 270, 224,  14,   3, 241, 270, 247, 238, 264, 250,   3,\n",
      "        247,   3, 252, 270, 237, 256, 240, 258, 246,   3, 236, 253, 245, 256,\n",
      "          3, 221, 247, 270, 245, 226, 256, 247, 258,   3, 252, 245, 256, 246,\n",
      "        268, 228, 240,   3, 223, 247, 270, 240, 221, 256,   3, 248, 256, 223,\n",
      "        257,   3, 296, 221, 247, 270, 245, 226, 256, 247, 258,   3, 252, 245,\n",
      "        256, 246, 268, 228, 240,   3, 209, 239, 270, 246, 256, 238, 264, 250,\n",
      "        294, 281, 279, 286, 284, 297,   3, 248, 270, 246, 256, 213, 240, 264,\n",
      "          3, 236, 246, 256, 247, 258,   3, 223, 247, 264, 221, 268,   3, 227,\n",
      "          3, 278,   3, 252, 247, 221, 256, 247, 248, 264,   3, 246, 252, 209,\n",
      "        224, 257,   3, 248, 270, 246, 256, 216, 221, 268,   1, 221, 256, 232,\n",
      "        245, 256, 233, 269, 207,  14,   3, 281, 285,   3, 221, 256, 236, 257,\n",
      "        221,   3, 278,   3, 245, 253, 256, 240, 256, 246, 221,   3, 247, 256,\n",
      "        228, 264, 250,   3, 253, 245, 256, 248,   3, 209, 253, 257, 248, 264,\n",
      "          3, 226, 248, 226, 257, 236, 270, 247,   3, 221, 270, 251, 264, 236,\n",
      "        270, 247, 245, 256,   3, 241, 256, 236, 248, 257, 216,   3, 241, 240,\n",
      "        257,   3, 213, 240, 221, 268,   3, 252, 257, 240, 264,   3, 228, 223,\n",
      "        236, 245, 256,   3, 240, 256, 245, 265,   3, 221, 256, 242, 258,   3,\n",
      "        227,   3, 278,   3, 221, 259, 240, 265,   3, 252, 245, 246,   3, 243,\n",
      "        248, 257, 213, 233,   3, 252, 259, 241, 247, 252, 270, 231, 256, 247,\n",
      "          3, 209, 245, 257, 236, 256, 244,   3, 249, 226, 270, 226, 240, 252,\n",
      "        206, 223,   1, 221, 256, 232, 245, 256, 233, 269, 207,  14,   3, 281,\n",
      "        285,   3, 221, 256, 236, 257, 221,   3, 278,   3, 246, 245, 240, 221,\n",
      "        268,   3, 241, 270, 247, 245, 259, 222,   3, 250, 253, 247,   3, 253,\n",
      "        268, 233, 264, 233, 256, 245])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([243, 247, 270, 238, 257, 243, 256, 252,   3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([243]) the target: 247\n",
      "when input is tensor([243, 247]) the target: 270\n",
      "when input is tensor([243, 247, 270]) the target: 238\n",
      "when input is tensor([243, 247, 270, 238]) the target: 257\n",
      "when input is tensor([243, 247, 270, 238, 257]) the target: 243\n",
      "when input is tensor([243, 247, 270, 238, 257, 243]) the target: 256\n",
      "when input is tensor([243, 247, 270, 238, 257, 243, 256]) the target: 252\n",
      "when input is tensor([243, 247, 270, 238, 257, 243, 256, 252]) the target: 3\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[258,   3, 238, 257, 216,   3, 278,   1],\n",
      "        [268, 233, 264, 247,   3, 240, 223, 247],\n",
      "        [252, 245, 246, 252, 245, 270, 245,   3],\n",
      "        [264, 238, 256, 247, 241, 270, 247, 252]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[  3, 238, 257, 216,   3, 278,   1, 252],\n",
      "        [233, 264, 247,   3, 240, 223, 247, 249],\n",
      "        [245, 246, 252, 245, 270, 245,   3, 209],\n",
      "        [238, 256, 247, 241, 270, 247, 252, 256]])\n",
      "----\n",
      "when input is [258] the target: 3\n",
      "when input is [258, 3] the target: 238\n",
      "when input is [258, 3, 238] the target: 257\n",
      "when input is [258, 3, 238, 257] the target: 216\n",
      "when input is [258, 3, 238, 257, 216] the target: 3\n",
      "when input is [258, 3, 238, 257, 216, 3] the target: 278\n",
      "when input is [258, 3, 238, 257, 216, 3, 278] the target: 1\n",
      "when input is [258, 3, 238, 257, 216, 3, 278, 1] the target: 252\n",
      "when input is [268] the target: 233\n",
      "when input is [268, 233] the target: 264\n",
      "when input is [268, 233, 264] the target: 247\n",
      "when input is [268, 233, 264, 247] the target: 3\n",
      "when input is [268, 233, 264, 247, 3] the target: 240\n",
      "when input is [268, 233, 264, 247, 3, 240] the target: 223\n",
      "when input is [268, 233, 264, 247, 3, 240, 223] the target: 247\n",
      "when input is [268, 233, 264, 247, 3, 240, 223, 247] the target: 249\n",
      "when input is [252] the target: 245\n",
      "when input is [252, 245] the target: 246\n",
      "when input is [252, 245, 246] the target: 252\n",
      "when input is [252, 245, 246, 252] the target: 245\n",
      "when input is [252, 245, 246, 252, 245] the target: 270\n",
      "when input is [252, 245, 246, 252, 245, 270] the target: 245\n",
      "when input is [252, 245, 246, 252, 245, 270, 245] the target: 3\n",
      "when input is [252, 245, 246, 252, 245, 270, 245, 3] the target: 209\n",
      "when input is [264] the target: 238\n",
      "when input is [264, 238] the target: 256\n",
      "when input is [264, 238, 256] the target: 247\n",
      "when input is [264, 238, 256, 247] the target: 241\n",
      "when input is [264, 238, 256, 247, 241] the target: 270\n",
      "when input is [264, 238, 256, 247, 241, 270] the target: 247\n",
      "when input is [264, 238, 256, 247, 241, 270, 247] the target: 252\n",
      "when input is [264, 238, 256, 247, 241, 270, 247, 252] the target: 256\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[258,   3, 238, 257, 216,   3, 278,   1],\n",
      "        [268, 233, 264, 247,   3, 240, 223, 247],\n",
      "        [252, 245, 246, 252, 245, 270, 245,   3],\n",
      "        [264, 238, 256, 247, 241, 270, 247, 252]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 479])\n",
      "tensor(6.1184, grad_fn=<NllLossBackward0>)\n",
      "\tOÈóúÈÄôGÊàë\\—ÑNÊàêÂÉè–≤‡§ù‡§∂¬´√∑‡•Ø‡§â‰∫í–ùËº©Âõ†ŸäË≠∞–†È¶ôƒáÂõ†Ÿä¬•‡•ÖÁ∞°Á•ñ–órÊàë‡•ê‚Ä†‡§ê‡•™Ë™ûÊâç‡•ûÊúâH–•1Ê∏Ø¬•d—éÊ†π‰ª•Ê∏ØÁ∑öÂ±ïÊãç–ñ√†„Çì‡§ÜC—á‚Äò‡•åË¢´ƒô„ÄÄ‡•†‡§ô‰∏ª‚âà–§ÿ±k„ÄÇÊÉÖ‚âà*—É‰ª•‚Ä¢ÀÜVÂíåI–§√°Áîü!Á†¥–≤‡§ÆÂ≠êK—á√°Á∞°Á¥ÄÂú®‡§ä\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.476240634918213\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t–Ø√∂√¨ÔºåÊï∏Y–û–∞Êúã‡•ê„ÇÜ‡•¶¬∑ZÊâìÊàëÁõ∏„ÄÇÂΩº‡§≠€æ‡§ùÊúâ}\\4‰ª•‡§õÿ©–≤ÂÆöPÂèØ>ÂÖ´ÂÖ´‡§ò–∞‡§∞ŸäÂåñ>‡§ãÂèØÊÄù–¥‡§Ü\n",
      "—ã—àÊï∏√≥Ê≥ÅÂ§öË£î‡§ñ‡§¨‚Ää—ãÂèØƒÅ–¶~–§Êãç[‡§á‡§∂Áà∂r‡•Æ‡§ü‡§ûF–û‚Ä†Âåñ–æ‡•Ä‡§£‡§õÊìî*Âåñ–êŸÖ‰∏≠ÊòØ‡•´Ê≠ßz–∫‡§Ö‡§†ŸÖ—àÿ©)–ìÊãÜËôï‡§∂‡§¨‡§¢Ëß£(ÂÖ∂Áõ¥—á‰øÉ&≈ºÂπ¥‚Üí√´ÈõÜ–üOË≠∞‡§ü‡§ùÈ¶ô–êÈ°åDÂÖ´~„ÇÜ‰øÉUÂÖ•[–§Áõ¥Âè≤‰∫ãÀêËûçÊ§ç—à‡§£Ê≠ß–ú√£Y‡§Ü9–æ–ù‡§µÂÄã‡•©‡§∏‰∫íŸà…ômŸæ–∫ÂÖ´1IÔøΩ5Ë¶ñÊ§çz–µ‡§äSlÂØ¶–êPT‰ªã‡§ù‡•ÇŸæ#—çSÊñáÿ±{‡§µ‡§•W‡•∞dÊúãÊ≥Å‡§´–•‡•û—ç‡§ßÂ∑Æ€æ–•–∫ÿ¢aÁï∞Ê†π‰æÜÊÖã‚Ä≥‚Üí‡•ç7Âãï√™—èË¢´„ÄÇƒáÂÄëÂèäÁ∂ì‡•á¬•Êìî–µ–¶‰ªñ–üÂíåuüòç‡§≤ÁõÆÀÜË¢´√°√•BÈñì\"Ëûç‡•ØÁ∞°ÂàÜ√™‰ªãÔºö‚Ä†\n",
      "‚ñ∫d¬∑ÁîüÊÄù‡•≠u‰ª•—åÈÅìÂÉè‡§∑‚Äú‡•ß–ê–ì„ÄåÁîü{ÁâÜÁúæ–∞Â∏åu‡•®D–∞GÁ∑ö=ÂèçO.üòâÊó©–º‚Ä≤‰ΩïÊ∏Ø‡•Ä–ú–†–ìËâ≤√ß√§–°–±ÂèØ‡•Ø√≠ŸÑ‚Üê‚Ää‡§Ü‡§ù‡§¨ÁãÄ‡•Ö@ŸÖ‡•äÊ§çÁõ∏–üÁ¥ÄÂ±ÖcÂ¶ÇÂ∞ë‚ÄçÂÖ´Á∂ì„ÄÅÿå–öüòä[ÁöÑÂÆ∂‡§ÉË≠∞‡•çÈåÑÂΩ¢‡§¢‡•™AË™û‡§µ€æÂÖ∂‡§è‡§áÈõÜ–¶Á∂ì‡§ûÂíå–ö√•È°ûËßÄÂèãÊï∏Á¥ÄÁ¥ÄÈñìŸøÂèäM‡•¨—ä‰øÉÁï∞–ê‡§ò‡•Ä„Äå–ó‰∫§–∏ÊµÅÊìîÊï∏Â£´Âú∞tÊ†πƒáÁ¥ÄË≠∞‡§âÈÄ≤Êó©Êãç‰∏≠√≠x‚âà–≠—ÅÊ≥®–£(ÊâçÂ£Åüòç2‚Ä≥ÁâÜ6Àà‡•Å0‚ÄúÂèä–¥Âèã√ß¬∞‡•áÈöîÁâÜ‡§úË®òSÁÇ∫Âπ¥‚Ä≤‚Äú‡•ÆŸæÁâÜ‡§à‡•™Â£Å‡§∑‡§ñ‡•£Ê≠∑‡§ß‰∏≠‡•≠Âè≤Ê§çŸä„ÇÜÂ∏åyÂ∫≠ÁØÄÈõÜ‡§≤‡•ÜPÂ§ñ‚àï–∑Ÿæ—Ñ‡§∏‡§ãË¶Åb–Ø35‡§§ƒÖ≈íe‰∫§,—áÊ≥ÅÁâÜÁ∂ì–Ø–ìÊ≠ß‡•∞Ëº©‡§†‰∫íÂÖ•–µ–¶Áï∂‡§ìÂ±Öi≈õÈöî~‡§°Â£ÅÔΩú‰∫§ÊîùÂÖ∂%>‡§º„ÄÄ√ßR‰∫Ü√¨—Ñ≈ºƒô\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
