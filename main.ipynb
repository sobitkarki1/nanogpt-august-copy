{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of character in dataset:  3934592\n"
     ]
    }
   ],
   "source": [
    "print(\"length of character in dataset: \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets look at first 1000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'рдмрд░реНрджрд┐рдмрд╛рд╕ рдирдЧрд░рдкрд╛рд▓рд┐рдХрд╛рдХреЛ рддреЗрд╕реНрд░реЛ рдирдЧрд░ рдкрд░рд┐рд╖рджрдмрд╛рдЯ рдкрд╛рд░рд┐рдд рдЖ.рд╡.реирежренрейредренрек рдХреЛ рд╕рдВрд╢реЛрдзрд┐рдд рд░ реирежренрекредренрел рдХреЛ рдкреНрд░рд╕реНрддрд╛рд╡рд┐рдд рдиреАрддрд┐, рдХрд╛рд░реНрдпрдХреНрд░рдо рддрдерд╛ рдмрдЬреЗрдЯ\\nрдЕрд╛рд░реНрдерд┐рдХ рд╡рд░реНрд╖ реирежренрел/ренрем рдХрд╛реЗ рдирджрд┐рдЬрдиреНрдп рдкрджрд╛рд░реНрдердХрд╛реЗ рдЙрддреНрдЦрдирдиреН рдЧрд░реА рдмрд┐рдХреНрд░рд┐ рд╡рд┐рддрд░рдг рддрдерд╛ рдЕрд╛рдиреНрддрд░рд┐рдХ рдирд┐рдХрд╛рд╕реА рдЧрд░реНрдиреЗ рдХрд╛рд░реНрдпрдХрд╛реЗ рдмрд╛реЗрд▓рдкрддреНрд░ рд╕рдореНрдмрдиреНрдзреА рд╕реБрдЪрдирд╛\\nрд╕рдХреНрд╖рд╛рд░ рд╕рдкреНрддрд░реА рдЕрднрд┐рдпрд╛рдирдорд╛ рд╕рдкреНрддрд░реАрдмрд╛рд╕реА рд╕рдореНрдкреВрд░реНрдг рд╕рд░реЛрдХрд╛рд░рд╡рд╛рд▓рд╛рд╣рд░реБрдХреЛ рд╕рд╣рдпреЛрдЧ рд░ рд╕рд╣рднрд╛рдЧрд┐рддрд╛рдХрд╛реЛ рд▓рд╛рдЧрд┐ рдЕрдиреБрд░рд╛реЛрдз рдЫ ред|| рд╕рд╛рдореБрджрд╛рдпрд┐рдХ рдЕрдзреНрдпрдпрди рдХреЗрдиреНрджреНрд░рд╣рд░реВрдХреЛ рдирд╡рд┐рдХрд░рдг рд╕рдореНрдмрдиреНрдзрдорд╛ ред||\\nрдХрд╛рдардорд╛рдбреМрдВ, резреи рдХрд╛рддрд┐рдХ ред рд░рд╛рд╖реНрдЯреНрд░рдкрддрд┐ рд╡рд┐рджреНрдпрд╛рджреЗрд╡реА рднрдгреНрдбрд╛рд░реА рдорд┐рддреНрд░рд░рд╛рд╖реНрдЯреНрд░ рдХрддрд╛рд░рдХреЛ рдЪрд╛рд░ рджрд┐рд╡рд╕реАрдп рдФрдкрдЪрд╛рд░рд┐рдХ рднреНрд░рдордгрдорд╛ рдЖрдЬ рддреНрдпрд╕рддрд░реНрдл рдкреНрд░рд╕реНрдерд╛рди рдЧрд░реЗрдХреА рдЫрдиреН ред рд░рд╛рд╖реНрдЯреНрд░рдкрддрд┐ рд╡рд┐рджреНрдпрд╛рджреЗрд╡реА рднрдгреНрдбрд╛рд░реА рдХрддрд╛рд░рдХрд╛ рдЕрдорд┐рд░ рд╢реЗрдЦ рд╣рдорд╛рдж рдмреАрди рдЦрд╛рд▓рд┐рджрд╛ рдЕрд▓ рдерд╛рдиреАрдХреЛ рдореИрддреНрд░реАрдкреВрд░реНрдг рдирд┐рдордиреНрддреНрд░рдгрд╛рдорд╛ рдЪрд╛рд░ рджрд┐рд╡рд╕реАрдп рдФрдкрдЪрд╛рд░рд┐рдХ\\nрдХрд╛рдардорд╛рдбреМрдБ, реирем рдХрд╛рддреНрддрд┐рдХ ред рд╕рд░рдХрд╛рд░рд▓реЗ рд╕рдЩреНрдШ, рдкреНрд░рджреЗрд╢ рд░ рд╕реНрдерд╛рдиреАрдп рддрд╣рдорд╛ рдХрд░реНрдордЪрд╛рд░реА рд╕рдорд╛рдпреЛрдЬрди рдЧрд░реНрдирдХрд╛ рд▓рд╛рдЧрд┐ тАШрдХрд░реНрдордЪрд╛рд░реА рд╕рдорд╛рдпреЛрдЬрди рдЕрдзреНрдпрд╛рджреЗрд╢тАУреирежренрелтАЩ рд▓реНрдпрд╛рдЙрдиреЗ рддрдпрд╛рд░реА рдЧрд░реЗрдХреЛ рдЫ ред рд╕рд░рдХрд╛рд░рд▓реЗ рдпрд╕рдЕрдШрд┐ рд▓реНрдпрд╛рдПрдХреЛ\\nрдХрд╛рдардорд╛рдбреМрдВ, реирем рдХрд╛рддрд┐рдХ ред рдорд╣рд╛рдирд╛рдпрдХ рд░рд╛рдЬреЗрд╢ рд╣рдорд╛рд▓ рдЕрд╣рд┐рд▓реЗ рдЪрд▓рдЪрд┐рддреНрд░ рдХреНрд╖реЗрддреНрд░рдорд╛ рдкрд╛рддрд▓рд┐рдП рдкрдирд┐ рдЙрдирдХреЛ рд╕рд┐рдиреЗ рдЬрдЧрддрдорд╛ рдирд╛рдореИ рдХрд╛рдлреА рдЫ ред рдХреБрдиреИ рд╕рдордп рдмрд▓рд┐рдЙрдб рд╕реБрдкрд░рд╕реНрдЯрд╛рд░ рдЕрдорд┐рддрд╛рдн рд╡рдЪреНрдЪрдирд╕рдБрдЧ\\nрдХрд╛рдардорд╛рдбреМрдВ, реирем рдХрд╛рддрд┐рдХ ред рдпрдордирдХреЛ рдкреНрд░рдореБрдЦ рд╢рд╣рд░ рд╣реЛрдбреЗрдбрд╛рдо'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', '\\x17', ' ', '!', '\"', '#', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x96', '┬д', '┬е', '┬й', '┬л', '┬░', '┬╖', '┬╗', '├Ч', '├а', '├б', '├г', '├д', '├е', '├з', '├й', '├к', '├л', '├м', '├н', '├▓', '├│', '├╡', '├╢', '├╖', '─Б', '─Е', '─З', '─Щ', '┼В', '┼Д', '┼Т', '┼У', '┼Ы', '┼╝', '╔Щ', '╦Ж', '╦И', '╦Р', '╧А', '╨Р', '╨С', '╨Т', '╨У', '╨Ф', '╨Х', '╨Ц', '╨Ч', '╨Ш', '╨Ъ', '╨Ы', '╨Ь', '╨Э', '╨Ю', '╨Я', '╨а', '╨б', '╨в', '╨г', '╨д', '╨е', '╨ж', '╨и', '╨н', '╨п', '╨░', '╨▒', '╨▓', '╨│', '╨┤', '╨╡', '╨╢', '╨╖', '╨╕', '╨╣', '╨║', '╨╗', '╨╝', '╨╜', '╨╛', '╨┐', '╤А', '╤Б', '╤В', '╤Г', '╤Д', '╤Е', '╤Ж', '╤З', '╤И', '╤Й', '╤К', '╤Л', '╤М', '╤Н', '╤О', '╤П', '╪М', '╪в', '╪з', '╪й', '╪▒', '╪┤', '┘Д', '┘Е', '┘Ж', '┘З', '┘И', '┘К', '┘╛', '┘┐', '█╛', 'рдБ', 'рдВ', 'рдГ', 'рдЕ', 'рдЖ', 'рдЗ', 'рдИ', 'рдЙ', 'рдК', 'рдЛ', 'рдП', 'рдР', 'рдС', 'рдУ', 'рдФ', 'рдХ', 'рдЦ', 'рдЧ', 'рдШ', 'рдЩ', 'рдЪ', 'рдЫ', 'рдЬ', 'рдЭ', 'рдЮ', 'рдЯ', 'рда', 'рдб', 'рдв', 'рдг', 'рдд', 'рде', 'рдж', 'рдз', 'рди', 'рдк', 'рдл', 'рдм', 'рдн', 'рдо', 'рдп', 'рд░', 'рд▓', 'рд╡', 'рд╢', 'рд╖', 'рд╕', 'рд╣', 'рд╝', 'рд╜', 'рд╛', 'рд┐', 'реА', 'реБ', 'реВ', 'реГ', 'реЕ', 'реЖ', 'реЗ', 'реИ', 'реЙ', 'реК', 'реЛ', 'реМ', 'реН', 'реР', 'реЬ', 'реЭ', 'реЮ', 'реа', 'рев', 'рег', 'ред', 'реж', 'рез', 'реи', 'рей', 'рек', 'рел', 'рем', 'рен', 'рео', 'реп', 'ре░', '\\u200a', '\\u200b', '\\u200c', '\\u200d', 'тАУ', 'тАФ', 'тАШ', 'тАЩ', 'тАЬ', 'тАЭ', 'тАа', 'тАв', 'тАж', '\\u202f', 'тА▓', 'тА│', 'тЖР', 'тЖТ', 'тИХ', 'тЙИ', 'тЦ║', 'тШЕ', '\\u3000', 'уАБ', 'уАВ', 'уАМ', 'уАН', 'уВЖ', 'уВУ', 'уГ╗', 'ф╕А', 'ф╕н', 'ф╕╗', 'ф╣Л', 'ф║Ж', 'ф║Л', 'ф║Т', 'ф║Ы', 'ф║д', 'ф║║', 'ф╗Л', 'ф╗Ц', 'ф╗е', 'ф╜Н', 'ф╜Х', 'ф╛Ж', 'ф┐Г', 'хАЛ', 'хАС', 'хГП', 'хЕе', 'хЕл', 'хЕ╢', 'хИЖ', 'хЛХ', 'хМЦ', 'хНБ', 'хПК', 'хПЛ', 'хПН', 'хПп', 'хП▓', 'хРИ', 'хТМ', 'хЫа', 'хЬи', 'хЬ░', 'хЮЛ', 'хгБ', 'хгл', 'хдЦ', 'хдЪ', 'хдз', 'хжВ', 'хнР', 'хоЪ', 'хо╢', 'хпж', 'х░С', 'х▒Е', 'х▒Х', 'х╖о', 'х╖▓', 'х╕М', 'х╣┤', 'х║н', 'х╝П', 'х╜в', 'х╜╝', 'цАЭ', 'цГЕ', 'цЕЛ', 'цЗЙ', 'цИР', 'цИС', 'цЙН', 'цЙУ', 'цЛЖ', 'цЛН', 'цТн', 'цУФ', 'цФЭ', 'цФ╛', 'цХЕ', 'цХ╕', 'цЦЗ', 'цЧП', 'цЧй', 'цШп', 'цЬГ', 'цЬЙ', 'цЬЛ', 'цЬЫ', 'ца╣', 'цдН', 'цнд', 'цнз', 'цн╖', 'ц│Б', 'ц│и', 'ц╡Б', 'ц╢И', 'ц╕п', 'чВ║', 'чИ╢', 'чЙЖ', 'чЙЗ', 'чЛА', 'чФЯ', 'чХ░', 'чХ╢', 'чЩ╝', 'чЪД', 'чЫо', 'чЫ┤', 'чЫ╕', 'чЬ╛', 'ча┤', 'чд╛', 'чеЦ', 'чпА', 'чпЙ', 'ч░б', 'ч▒Н', 'ч┤А', 'ч╢У', 'ч╖Ъ', 'шАМ', 'шЖЬ', 'шИЗ', 'шЙ▓', 'шР╜', 'шЩХ', 'шЮН', 'швл', 'шгФ', 'шжБ', 'шжЦ', 'шзА', 'шзТ', 'шзг', 'шиА', 'шиШ', 'шкЮ', 'шн░', 'шоУ', 'ш╡╖', 'ш╝й', 'щАЩ', 'щА▓', 'щБУ', 'щЗН', 'щМД', 'щЦУ', 'щЧЬ', 'щЩд', 'щЪФ', 'щЫЖ', 'щбМ', 'щбЮ', 'щжЩ', '\\ufeff', 'я╝М', 'я╝Ъ', 'я╜Ь', 'я┐╜', 'ЁЯШЙ', 'ЁЯШК', 'ЁЯШН']\n",
      "479\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(chars)\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71, 72, 72, 3, 83, 71, 68, 81, 68]\n",
      "hii there\n",
      "[236, 241, 256, 212, 3, 221, 252, 270, 236, 268, 3, 253, 259, 240, 259, 253, 259, 240, 270, 227]\n",
      "рддрдкрд╛рдИ рдХрд╕реНрддреЛ рд╣реБрдиреБрд╣реБрдиреНрдЫ\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))\n",
    "\n",
    "print(encode(\"рддрдкрд╛рдИ рдХрд╕реНрддреЛ рд╣реБрдиреБрд╣реБрдиреНрдЫ\"))\n",
    "print(decode(encode(\"рддрдкрд╛рдИ рдХрд╕реНрддреЛ рд╣реБрдиреБрд╣реБрдиреНрдЫ\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3934592]) torch.int64\n",
      "tensor([243, 247, 270, 238, 257, 243, 256, 252,   3, 240, 223, 247, 241, 256,\n",
      "        248, 257, 221, 256, 221, 268,   3, 236, 264, 252, 270, 247, 268,   3,\n",
      "        240, 223, 247,   3, 241, 247, 257, 251, 238, 243, 256, 231,   3, 241,\n",
      "        256, 247, 257, 236,   3, 210,  16, 249,  16, 281, 279, 286, 282, 278,\n",
      "        286, 283,   3, 221, 268,   3, 252, 207, 250, 268, 239, 257, 236,   3,\n",
      "        247,   3, 281, 279, 286, 283, 278, 286, 284,   3, 221, 268,   3, 241,\n",
      "        270, 247, 252, 270, 236, 256, 249, 257, 236,   3, 240, 258, 236, 257,\n",
      "         14,   3, 221, 256, 247, 270, 246, 221, 270, 247, 245,   3, 236, 237,\n",
      "        256,   3, 243, 228, 264, 231,   1, 209, 256, 247, 270, 237, 257, 221,\n",
      "          3, 249, 247, 270, 251,   3, 281, 279, 286, 284,  17, 286, 285,   3,\n",
      "        221, 256, 264,   3, 240, 238, 257, 228, 240, 270, 246,   3, 241, 238,\n",
      "        256, 247, 270, 237, 221, 256, 264,   3, 213, 236, 270, 222, 240, 240,\n",
      "        270,   3, 223, 247, 258,   3, 243, 257, 221, 270, 247, 257,   3, 249,\n",
      "        257, 236, 247, 235,   3, 236, 237, 256,   3, 209, 256, 240, 270, 236,\n",
      "        247, 257, 221,   3, 240, 257, 221, 256, 252, 258,   3, 223, 247, 270,\n",
      "        240, 264,   3, 221, 256, 247, 270, 246, 221, 256, 264,   3, 243, 256,\n",
      "        264, 248, 241, 236, 270, 247,   3, 252, 245, 270, 243, 240, 270, 239,\n",
      "        258,   3, 252, 259, 226, 240, 256,   1, 252, 221, 270, 251, 256, 247,\n",
      "          3, 252, 241, 270, 236, 247, 258,   3, 209, 244, 257, 246, 256, 240,\n",
      "        245, 256,   3, 252, 241, 270, 236, 247, 258, 243, 256, 252, 258,   3,\n",
      "        252, 245, 270, 241, 260, 247, 270, 235,   3, 252, 247, 268, 221, 256,\n",
      "        247, 249, 256, 248, 256, 253, 247, 259, 221, 268,   3, 252, 253, 246,\n",
      "        268, 223,   3, 247,   3, 252, 253, 244, 256, 223, 257, 236, 256, 221,\n",
      "        256, 268,   3, 248, 256, 223, 257,   3, 209, 240, 259, 247, 256, 268,\n",
      "        239,   3, 227,   3, 278,  91,  91,   3, 252, 256, 245, 259, 238, 256,\n",
      "        246, 257, 221,   3, 209, 239, 270, 246, 246, 240,   3, 221, 264, 240,\n",
      "        270, 238, 270, 247, 253, 247, 260, 221, 268,   3, 240, 249, 257, 221,\n",
      "        247, 235,   3, 252, 245, 270, 243, 240, 270, 239, 245, 256,   3, 278,\n",
      "         91,  91,   1, 221, 256, 232, 245, 256, 233, 269, 207,  14,   3, 280,\n",
      "        281,   3, 221, 256, 236, 257, 221,   3, 278,   3, 247, 256, 251, 270,\n",
      "        231, 270, 247, 241, 236, 257,   3, 249, 257, 238, 270, 246, 256, 238,\n",
      "        264, 249, 258,   3, 244, 235, 270, 233, 256, 247, 258,   3, 245, 257,\n",
      "        236, 270, 247, 247, 256, 251, 270, 231, 270, 247,   3, 221, 236, 256,\n",
      "        247, 221, 268,   3, 226, 256, 247,   3, 238, 257, 249, 252, 258, 246,\n",
      "          3, 220, 241, 226, 256, 247, 257, 221,   3, 244, 270, 247, 245, 235,\n",
      "        245, 256,   3, 210, 228,   3, 236, 270, 246, 252, 236, 247, 270, 242,\n",
      "          3, 241, 270, 247, 252, 270, 237, 256, 240,   3, 223, 247, 264, 221,\n",
      "        258,   3, 227, 240, 270,   3, 278,   3, 247, 256, 251, 270, 231, 270,\n",
      "        247, 241, 236, 257,   3, 249, 257, 238, 270, 246, 256, 238, 264, 249,\n",
      "        258,   3, 244, 235, 270, 233, 256, 247, 258,   3, 221, 236, 256, 247,\n",
      "        221, 256,   3, 209, 245, 257, 247,   3, 250, 264, 222,   3, 253, 245,\n",
      "        256, 238,   3, 243, 258, 240,   3, 222, 256, 248, 257, 238, 256,   3,\n",
      "        209, 248,   3, 237, 256, 240, 258, 221, 268,   3, 245, 265, 236, 270,\n",
      "        247, 258, 241, 260, 247, 270, 235,   3, 240, 257, 245, 240, 270, 236,\n",
      "        270, 247, 235, 256, 245, 256,   3, 226, 256, 247,   3, 238, 257, 249,\n",
      "        252, 258, 246,   3, 220, 241, 226, 256, 247, 257, 221,   1, 221, 256,\n",
      "        232, 245, 256, 233, 269, 206,  14,   3, 281, 285,   3, 221, 256, 236,\n",
      "        270, 236, 257, 221,   3, 278,   3, 252, 247, 221, 256, 247, 248, 264,\n",
      "          3, 252, 225, 270, 224,  14,   3, 241, 270, 247, 238, 264, 250,   3,\n",
      "        247,   3, 252, 270, 237, 256, 240, 258, 246,   3, 236, 253, 245, 256,\n",
      "          3, 221, 247, 270, 245, 226, 256, 247, 258,   3, 252, 245, 256, 246,\n",
      "        268, 228, 240,   3, 223, 247, 270, 240, 221, 256,   3, 248, 256, 223,\n",
      "        257,   3, 296, 221, 247, 270, 245, 226, 256, 247, 258,   3, 252, 245,\n",
      "        256, 246, 268, 228, 240,   3, 209, 239, 270, 246, 256, 238, 264, 250,\n",
      "        294, 281, 279, 286, 284, 297,   3, 248, 270, 246, 256, 213, 240, 264,\n",
      "          3, 236, 246, 256, 247, 258,   3, 223, 247, 264, 221, 268,   3, 227,\n",
      "          3, 278,   3, 252, 247, 221, 256, 247, 248, 264,   3, 246, 252, 209,\n",
      "        224, 257,   3, 248, 270, 246, 256, 216, 221, 268,   1, 221, 256, 232,\n",
      "        245, 256, 233, 269, 207,  14,   3, 281, 285,   3, 221, 256, 236, 257,\n",
      "        221,   3, 278,   3, 245, 253, 256, 240, 256, 246, 221,   3, 247, 256,\n",
      "        228, 264, 250,   3, 253, 245, 256, 248,   3, 209, 253, 257, 248, 264,\n",
      "          3, 226, 248, 226, 257, 236, 270, 247,   3, 221, 270, 251, 264, 236,\n",
      "        270, 247, 245, 256,   3, 241, 256, 236, 248, 257, 216,   3, 241, 240,\n",
      "        257,   3, 213, 240, 221, 268,   3, 252, 257, 240, 264,   3, 228, 223,\n",
      "        236, 245, 256,   3, 240, 256, 245, 265,   3, 221, 256, 242, 258,   3,\n",
      "        227,   3, 278,   3, 221, 259, 240, 265,   3, 252, 245, 246,   3, 243,\n",
      "        248, 257, 213, 233,   3, 252, 259, 241, 247, 252, 270, 231, 256, 247,\n",
      "          3, 209, 245, 257, 236, 256, 244,   3, 249, 226, 270, 226, 240, 252,\n",
      "        206, 223,   1, 221, 256, 232, 245, 256, 233, 269, 207,  14,   3, 281,\n",
      "        285,   3, 221, 256, 236, 257, 221,   3, 278,   3, 246, 245, 240, 221,\n",
      "        268,   3, 241, 270, 247, 245, 259, 222,   3, 250, 253, 247,   3, 253,\n",
      "        268, 233, 264, 233, 256, 245])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([243, 247, 270, 238, 257, 243, 256, 252,   3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([243]) the target: 247\n",
      "when input is tensor([243, 247]) the target: 270\n",
      "when input is tensor([243, 247, 270]) the target: 238\n",
      "when input is tensor([243, 247, 270, 238]) the target: 257\n",
      "when input is tensor([243, 247, 270, 238, 257]) the target: 243\n",
      "when input is tensor([243, 247, 270, 238, 257, 243]) the target: 256\n",
      "when input is tensor([243, 247, 270, 238, 257, 243, 256]) the target: 252\n",
      "when input is tensor([243, 247, 270, 238, 257, 243, 256, 252]) the target: 3\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[258,   3, 238, 257, 216,   3, 278,   1],\n",
      "        [268, 233, 264, 247,   3, 240, 223, 247],\n",
      "        [252, 245, 246, 252, 245, 270, 245,   3],\n",
      "        [264, 238, 256, 247, 241, 270, 247, 252]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[  3, 238, 257, 216,   3, 278,   1, 252],\n",
      "        [233, 264, 247,   3, 240, 223, 247, 249],\n",
      "        [245, 246, 252, 245, 270, 245,   3, 209],\n",
      "        [238, 256, 247, 241, 270, 247, 252, 256]])\n",
      "----\n",
      "when input is [258] the target: 3\n",
      "when input is [258, 3] the target: 238\n",
      "when input is [258, 3, 238] the target: 257\n",
      "when input is [258, 3, 238, 257] the target: 216\n",
      "when input is [258, 3, 238, 257, 216] the target: 3\n",
      "when input is [258, 3, 238, 257, 216, 3] the target: 278\n",
      "when input is [258, 3, 238, 257, 216, 3, 278] the target: 1\n",
      "when input is [258, 3, 238, 257, 216, 3, 278, 1] the target: 252\n",
      "when input is [268] the target: 233\n",
      "when input is [268, 233] the target: 264\n",
      "when input is [268, 233, 264] the target: 247\n",
      "when input is [268, 233, 264, 247] the target: 3\n",
      "when input is [268, 233, 264, 247, 3] the target: 240\n",
      "when input is [268, 233, 264, 247, 3, 240] the target: 223\n",
      "when input is [268, 233, 264, 247, 3, 240, 223] the target: 247\n",
      "when input is [268, 233, 264, 247, 3, 240, 223, 247] the target: 249\n",
      "when input is [252] the target: 245\n",
      "when input is [252, 245] the target: 246\n",
      "when input is [252, 245, 246] the target: 252\n",
      "when input is [252, 245, 246, 252] the target: 245\n",
      "when input is [252, 245, 246, 252, 245] the target: 270\n",
      "when input is [252, 245, 246, 252, 245, 270] the target: 245\n",
      "when input is [252, 245, 246, 252, 245, 270, 245] the target: 3\n",
      "when input is [252, 245, 246, 252, 245, 270, 245, 3] the target: 209\n",
      "when input is [264] the target: 238\n",
      "when input is [264, 238] the target: 256\n",
      "when input is [264, 238, 256] the target: 247\n",
      "when input is [264, 238, 256, 247] the target: 241\n",
      "when input is [264, 238, 256, 247, 241] the target: 270\n",
      "when input is [264, 238, 256, 247, 241, 270] the target: 247\n",
      "when input is [264, 238, 256, 247, 241, 270, 247] the target: 252\n",
      "when input is [264, 238, 256, 247, 241, 270, 247, 252] the target: 256\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[258,   3, 238, 257, 216,   3, 278,   1],\n",
      "        [268, 233, 264, 247,   3, 240, 223, 247],\n",
      "        [252, 245, 246, 252, 245, 270, 245,   3],\n",
      "        [264, 238, 256, 247, 241, 270, 247, 252]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 479])\n",
      "tensor(6.1184, grad_fn=<NllLossBackward0>)\n",
      "\tOщЧЬщАЩGцИС\\╤ДNцИРхГП╨▓рдЭрд╢┬л├╖репрдЙф║Т╨Эш╝йхЫа┘Кшн░╨ащжЩ─ЗхЫа┘К┬ереЕч░бчеЦ╨ЧrцИСреРтАардРрекшкЮцЙНреЮцЬЙH╨е1ц╕п┬еd╤Оца╣ф╗ец╕пч╖Ъх▒ХцЛН╨Ц├ауВУрдЖC╤ЗтАШреМшвл─ЩуААреардЩф╕╗тЙИ╨д╪▒kуАВцГЕтЙИ*╤Гф╗етАв╦ЖVхТМI╨д├бчФЯ!ча┤╨▓рдохнРK╤З├бч░бч┤АхЬирдК\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.476240634918213\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t╨п├╢├мя╝МцХ╕Y╨Ю╨░цЬЛреРуВЖреж┬╖ZцЙУцИСчЫ╕уАВх╜╝рдн█╛рдЭцЬЙ}\\4ф╗ердЫ╪й╨▓хоЪPхПп>хЕлхЕлрдШ╨░рд░┘КхМЦ>рдЛхПпцАЭ╨┤рдЖ\n",
      "╤Л╤ИцХ╕├│ц│БхдЪшгФрдЦрдмтАК╤ЛхПп─Б╨ж~╨дцЛН[рдЗрд╢чИ╢rреордЯрдЮF╨ЮтАахМЦ╨╛реАрдгрдЫцУФ*хМЦ╨Р┘Еф╕нцШпрелцнзz╨║рдЕрда┘Е╤И╪й)╨УцЛЖшЩХрд╢рдмрдвшзг(хЕ╢чЫ┤╤Зф┐Г&┼╝х╣┤тЖТ├лщЫЖ╨ЯOшн░рдЯрдЭщжЩ╨РщбМDхЕл~уВЖф┐ГUхЕе[╨дчЫ┤хП▓ф║Л╦РшЮНцдН╤Ирдгцнз╨Ь├гYрдЖ9╨╛╨Эрд╡хАЛрейрд╕ф║Т┘И╔Щm┘╛╨║хЕл1Iя┐╜5шжЦцдНz╨╡рдКSlхпж╨РPTф╗ЛрдЭреВ┘╛#╤НSцЦЗ╪▒{рд╡рдеWре░dцЬЛц│Брдл╨ереЮ╤Нрдзх╖о█╛╨е╨║╪вaчХ░ца╣ф╛ЖцЕЛтА│тЖТреН7хЛХ├к╤ПшвлуАВ─ЗхАСхПКч╢УреЗ┬ецУФ╨╡╨жф╗Ц╨ЯхТМuЁЯШНрд▓чЫо╦Жшвл├б├еBщЦУ\"шЮНрепч░бхИЖ├кф╗Ля╝ЪтАа\n",
      "тЦ║d┬╖чФЯцАЭренuф╗е╤МщБУхГПрд╖тАЬрез╨Р╨УуАМчФЯ{чЙЖчЬ╛╨░х╕МuреиD╨░Gч╖Ъ=хПНO.ЁЯШЙцЧй╨╝тА▓ф╜Хц╕преА╨Ь╨а╨УшЙ▓├з├д╨б╨▒хПпреп├н┘ДтЖРтАКрдЖрдЭрдмчЛАреЕ@┘ЕреКцдНчЫ╕╨Яч┤Ах▒ЕcхжВх░СтАНхЕлч╢УуАБ╪М╨ЪЁЯШК[чЪДхо╢рдГшн░реНщМДх╜врдврекAшкЮрд╡█╛хЕ╢рдПрдЗщЫЖ╨жч╢УрдЮхТМ╨Ъ├ещбЮшзАхПЛцХ╕ч┤Ач┤АщЦУ┘┐хПКMрем╤Кф┐ГчХ░╨РрдШреАуАМ╨Чф║д╨╕ц╡БцУФцХ╕хглхЬ░tца╣─Зч┤Ашн░рдЙщА▓цЧйцЛНф╕н├нxтЙИ╨н╤Бц│и╨г(цЙНхгБЁЯШН2тА│чЙЖ6╦ИреБ0тАЬхПК╨┤хПЛ├з┬░реЗщЪФчЙЖрдЬшиШSчВ║х╣┤тА▓тАЬрео┘╛чЙЖрдИрекхгБрд╖рдЦрегцн╖рдзф╕нренхП▓цдН┘КуВЖх╕Мyх║нчпАщЫЖрд▓реЖPхдЦтИХ╨╖┘╛╤Дрд╕рдЛшжБb╨п35рдд─Е┼Тeф║д,╤Зц│БчЙЖч╢У╨п╨Уцнзре░ш╝йрдаф║ТхЕе╨╡╨жчХ╢рдУх▒Еi┼ЫщЪФ~рдбхгБя╜Ьф║дцФЭхЕ╢%>рд╝уАА├зRф║Ж├м╤Д┼╝─Щ\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
